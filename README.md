ì‚¬ìš©ë²•
1. environment settings
   our reproduce is conducted with miniconda
```
conda create -n dreambooth python=3.10
conda activate dreambooth
pip install -r requirements.txt
```
2. file structure
```
dreambooth_project/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml        # train config
â”‚   â”œâ”€â”€ infer.yaml       # inference config
â”‚   â”œâ”€â”€ base_cap.yaml       # captioned train config
â”‚   â””â”€â”€ infer_cap.yaml       # captioned trained model inference config
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ instance/       # instance images
â”‚   â”‚    â”œâ”€â”€ dog/       
â”‚   â”‚    â””â”€â”€ mug/
â”‚   â””â”€â”€ class/          # auto generated path for prior preservation loss image  
â”‚        â”œâ”€â”€ dog/
â”‚        â””â”€â”€ mug/
â”œâ”€â”€ original_paper_experiments/    #codes for reproducting original dreambooth experiments
â”œâ”€â”€ our_own_experiments/    #codes for our own experiments
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ (auto generated paths)
â”œâ”€â”€ generate_captions.py #inference with image captioned prompt model
â”œâ”€â”€ inference.py        #inference 
â”œâ”€â”€ train_captioned.py  #train with image captioned prompt
â””â”€â”€ train.py            #train 

```
3. train

1.configs setting (configs/base.yaml)
```
python train.py --config_base path/to/config.yaml
ex) python train.py --config_base configs/base.yaml
```
Checkpoints and validation images will be automatically saved under the outputs/ directory.
The output folder name is determined by the experiment_name specified in the YAML config.

4. train with prompting(our improvement code)

1.configs setting (configs/base_cap.yaml)
2.make metadata.jsonl (ex: data/instance/teapot/metadata.jsonl)
```
{"file_name": "00.jpg", "text": "a photo of a sks teapot and a rose on a plate"}
{"file_name": "01.jpg", "text": "a photo of is a sks teapot and a cup on a wooden table"}
{"file_name": "02.jpg", "text": "a photo of is a sks teapot and a cup on a table"}
{"file_name": "03.jpg", "text": "a photo of is a sks teapot and a cup on a table"}
{"file_name": "04.jpg", "text": "a photo of is a sks teapot and a cup on a table"}
```
it can be generated with BLIP captioning
```
python generate_captions.py --image_dir path/to/subject_dir
ex) python generate_captions.py --image_dir data/instance/teapot
```
ìº¡ì…”ë‹ generate í›„ì— metadata.jsonl íŒŒì¼ì— identifier(ex: sks)ë¥¼ ì¶”ê°€í•˜ê³ , CLIP ì¸ì½”ë”ì— ì í•©í•˜ê²Œ a photo of ë¡œ ì‹œì‘í•˜ë„ë¡ ìˆ˜ì •
```
{"file_name": "01.jpg", "text": "there is a tea pot and a cup on a wooden table"}
=>
{"file_name": "01.jpg", "text": "a photo of is a sks teapot and a cup on a wooden table"}
```
ì´í›„ ë‹¤ìŒ ì½”ë“œë¡œ train í•œë‹¤.
```
python train_captioned.py --config_base path/to/configs.yaml
ex) python train_captioned.py --config_base configs/base_cap.yaml
```

6. inference
 
1.configs setting (configs/infer.yaml)
```
python inference.py --config_base path/to/config.yaml
ex) python inference.py --config_base configs/infer.yaml
```

## ğŸ“ Dataset Information

This implementation uses the official dataset provided by the authors of the DreamBooth paper:

**Repository:** https://github.com/google/dreambooth-dataset  
**Paper:** [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation (CVPR 2023)](https://arxiv.org/abs/2208.12242)

The dataset includes 30 subjects across 15 different classes, as used in the paper.

Some images were captured by the authors, and others were sourced from [Unsplash](https://unsplash.com).  
Attribution and license information is provided in:
