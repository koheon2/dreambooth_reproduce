
1. environment settings

Our reproduction was conducted using Miniconda.
```
conda create -n dreambooth python=3.10
conda activate dreambooth
pip install -r requirements.txt
```
2. file structure
```
dreambooth_project/
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îú‚îÄ‚îÄ base.yaml        # train config
‚îÇ   ‚îú‚îÄ‚îÄ infer.yaml       # inference config
‚îÇ   ‚îú‚îÄ‚îÄ base_cap.yaml       # captioned train config
‚îÇ   ‚îî‚îÄ‚îÄ infer_cap.yaml       # captioned trained model inference config
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ instance/       # instance images
‚îÇ   ‚îÇ    ‚îú‚îÄ‚îÄ dog/       
‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ mug/
‚îÇ   ‚îî‚îÄ‚îÄ class/          # auto generated path for prior preservation loss image  
‚îÇ        ‚îú‚îÄ‚îÄ dog/
‚îÇ        ‚îî‚îÄ‚îÄ mug/
‚îú‚îÄ‚îÄ original_paper_experiments/    #codes for reproducting original dreambooth experiments
‚îú‚îÄ‚îÄ our_own_experiments/    #codes for our own experiments
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îî‚îÄ‚îÄ (auto generated paths)
‚îú‚îÄ‚îÄ generate_captions.py #inference with image captioned prompt model
‚îú‚îÄ‚îÄ inference.py        #inference 
‚îú‚îÄ‚îÄ train_captioned.py  #train with image captioned prompt
‚îî‚îÄ‚îÄ train.py            #train 

```
3. train

1.configs setting (configs/base.yaml)
```
python train.py --config_base path/to/config.yaml
ex) python train.py --config_base configs/base.yaml
```
Checkpoints and validation images will be automatically saved under the outputs/ directory.
The output folder name is determined by the experiment_name specified in the YAML config.

4. train with prompting(our improvement code)

1.configs setting (configs/base_cap.yaml)
2.make metadata.jsonl (ex: data/instance/teapot/metadata.jsonl)
```
{"file_name": "00.jpg", "text": "a photo of a sks teapot and a rose on a plate"}
{"file_name": "01.jpg", "text": "a photo of is a sks teapot and a cup on a wooden table"}
{"file_name": "02.jpg", "text": "a photo of is a sks teapot and a cup on a table"}
{"file_name": "03.jpg", "text": "a photo of is a sks teapot and a cup on a table"}
{"file_name": "04.jpg", "text": "a photo of is a sks teapot and a cup on a table"}
```
It can be generated using BLIP captioning.
```
python generate_captions.py --image_dir path/to/subject_dir
ex) python generate_captions.py --image_dir data/instance/teapot
```
After generating captions, we post-process the metadata.jsonl file by adding an identifier (e.g., sks) to each entry and modifying the captions to begin with "a photo of" to ensure compatibility with the CLIP encoder.
```
{"file_name": "01.jpg", "text": "there is a tea pot and a cup on a wooden table"}
=>
{"file_name": "01.jpg", "text": "a photo of is a sks teapot and a cup on a wooden table"}
```
Then, start training using the following command:.
```
python train_captioned.py --config_base path/to/configs.yaml
ex) python train_captioned.py --config_base configs/base_cap.yaml
```

6. inference
 
1.configs setting (configs/infer.yaml)
```
python inference.py --config_base path/to/config.yaml
ex) python inference.py --config_base configs/infer.yaml
```

## üìÅ Dataset Information

This implementation uses the official dataset provided by the authors of the DreamBooth paper:

**Repository:** https://github.com/google/dreambooth-dataset  
**Paper:** [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation (CVPR 2023)](https://arxiv.org/abs/2208.12242)

The dataset includes 30 subjects across 15 different classes, as used in the paper.

Some images were captured by the authors, and others were sourced from [Unsplash](https://unsplash.com).  
Attribution and license information is provided in:
