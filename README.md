ì‚¬ìš©ë²•
1. í™˜ê²½ ì¤€ë¹„
```
# miniconda í™˜ê²½ ì˜ˆì‹œ
conda create -n dreambooth python=3.10
conda activate dreambooth
pip install -r requirements.txt
```
2. í´ë” êµ¬ì¡°
```
dreambooth_project/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml        # train config
â”‚   â”œâ”€â”€ infer.yaml       # inference config
â”‚   â”œâ”€â”€ base_cap.yaml       # captioned train config
â”‚   â””â”€â”€ infer_cap.yaml       # captioned trained model inference config
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ instance/       # instance images
â”‚   â”‚    â”œâ”€â”€ dog/       
â”‚   â”‚    â””â”€â”€ mug/
â”‚   â””â”€â”€ class/          # auto generated path for prior preservation loss image  
â”‚        â”œâ”€â”€ dog/
â”‚        â””â”€â”€ mug/
â”œâ”€â”€ original_paper_experiments/    #codes for reproducting original dreambooth experiments
â”œâ”€â”€ our_own_experiments/    #codes for our own experiments
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ (auto generated paths)
â”œâ”€â”€ generate_captions.py #inference with image captioned prompt model
â”œâ”€â”€ inference.py        #inference 
â”œâ”€â”€ train_captioned.py  #train with image captioned prompt
â””â”€â”€ train.py            #train 

```
3. train

ë¨¼ì € í•™ìŠµìš© config ì‘ì„± (ì˜ˆì‹œ: configs/base.yaml)
```
python -m scripts.train_dreambooth --config_base yamlíŒŒì¼ê²½ë¡œ
ex) python -m scripts.train_dreambooth --config_base configs/base.yaml

outputs/í´ë”/ ê²½ë¡œì— ì²´í¬í¬ì¸íŠ¸, validation ì´ë¯¸ì§€ê°€ ìë™ ìƒì„±. í´ë” ì´ë¦„ì€ yamlì—ì„œ ì§€ì •ë˜ëŠ” experiment_nameìœ¼ë¡œ ì„¤ì •ë¨
```

4. ì´ë¯¸ì§€ ìƒì„± (inference)
 
ë¨¼ì € inferenceìš© config ì‘ì„± (ì˜ˆì‹œ: configs/infer.yaml)
```
python -m scripts.inference --config_infer yamlíŒŒì¼ê²½ë¡œ
ex) python -m scripts.inference --config_infer configs/infer.yaml

outputs/ì‚¬ìš©í•œ ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” í´ë”/inference/ ê²½ë¡œë¡œ ì´ë¯¸ì§€ ì €ì¥
```
5. ì½”ë“œ ì£¼ìš” êµ¬ì¡°

config íŒŒì¼ë¡œ ëª¨ë“  ì‹¤í—˜ íŒŒë¼ë¯¸í„° ê´€ë¦¬ (ì‹¤í—˜ ìë™í™”, reproducibility)

data/instance/prompts_and_classes.txtì— ì—°êµ¬ìë“¤ì´ ì‚¬ìš©í–ˆë˜ í”„ë¡¬í”„íŠ¸ê°€ ì €ì¥ë˜ì–´ ìˆìŒ. 
ì´ í”„ë¡¬í”„íŠ¸ëŠ” imagen ê¸°ì¤€ì´ë¼ì„œ stable diffusionì˜ clipì— ì í•©í•œ í”„ë¡¬í”„íŠ¸ë¡œ í•´ì•¼ ì„±ëŠ¥ì´ ë” ì˜ ë‚˜ì˜¬ê²ƒ ê°™ì•„ìš”.

train:

validation ê¸°ëŠ¥ì€ í›ˆë ¨ì´ ì˜ ë˜ê³  ìˆëŠ”ì§€ í›ˆë ¨ ì¤‘ê°„ì— ì´ë¯¸ì§€ ìƒì„±í•´ì„œ ì²´í¬í•˜ëŠ” ê¸°ëŠ¥.

inference:

ì›í•˜ëŠ” checkpointë¡œ, ì—¬ëŸ¬ prompt/seed/batch/ìƒ˜í”ŒëŸ¬ ì‹¤í—˜ ì§€ì›

ì‹¤í—˜ë³„ í´ë”/ì´ë¯¸ì§€ëª… ìë™ ê´€ë¦¬

ê¸°íƒ€ ë“±ë“±...


## ğŸ“ Dataset Information

This implementation uses the official dataset provided by the authors of the DreamBooth paper:

**Repository:** https://github.com/google/dreambooth-dataset  
**Paper:** [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation (CVPR 2023)](https://arxiv.org/abs/2208.12242)

The dataset includes 30 subjects across 15 different classes, as used in the paper.

Some images were captured by the authors, and others were sourced from [Unsplash](https://unsplash.com).  
Attribution and license information is provided in:
